{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/UNSW-COMP9418/Week05/blob/main/COMP9418_W05_Markov_Chains_and_Hidden_Markov_Models.ipynb)\n",
    "\n",
    "# Markov Chains and Hidden Markov Models\n",
    "\n",
    "**COMP9418 W05 Tutorial**\n",
    "\n",
    "- Instructor: Gustavo Batista\n",
    "- School of Computer Science and Engineering, UNSW Sydney \n",
    "- Notebook designed by Gustavo Batista and Jeremy Gillen\n",
    "- Last Update 6th September 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this week's tutorial, we will explore models that use the Markovian assumptions, in particular Markov chains and Hidden Markov Models. We will implement the forward and Viterbi algorithms and use them to gain intuition about the convergence of Markov chains, and the probabilitic queries these algorithms can answer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical prerequisites\n",
    "\n",
    "You will need certain packages installed to run this notebook.\n",
    "\n",
    "If you are using ``conda``'s default\n",
    "[full installation](https://conda.io/docs/install/full.html),\n",
    "these requirements should all be satisfied already.\n",
    "\n",
    "If you are using ``virtualenv`` or other native package management,\n",
    "you may need to run these commands:\n",
    "\n",
    "```python\n",
    "pip3 install numpy matplotlib\n",
    "```\n",
    "\n",
    "To render a visualization of some graphical models, you also need to install Graphviz [download page](http://www.graphviz.org/download). We have already used this library in Tutorial 1, thus, you should have it installed. If you do not have it and use the conda installation, then use the command ```conda install python-graphviz```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have done all that, we\n",
    "import some useful modules for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from graphviz import Digraph\n",
    "# combinatorics\n",
    "from itertools import product, combinations\n",
    "\n",
    "from DiscreteFactors import Factor\n",
    "from Graph import Graph \n",
    "from BayesNet import BayesNet "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Chains\n",
    "\n",
    "Let's start implementing the mini-forward algorithm for Markov chains. We will use this algorithm to calculate the probability of a sequence of events as well as testing the convergence of some chains.\n",
    "\n",
    "Remember from the course slides that a Markov chain (as well as the Hidden Markov Model) is a Dynamic Bayesian Network (DBN). It means that this network \"grows\", i.e., we can add nodes as we iterate over time or space. We need to specify a notation to indicate time passage. Similarly to the slides, we use $t-1$ and $t$ appended to the variables names to designate the previous time and present time.\n",
    "\n",
    "We can use the example from the slides to introduce the notation and write the first transition tables.\n",
    "\n",
    "![](https://raw.githubusercontent.com/UNSW-COMP9418/Week05/main/img/weather.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define a Markov chain, we need to set the outcomeSpace as well as initial state and the transition probabilities. We will write the first two and let the third one as an exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# possible outcomes, by variable\n",
    "outcomeSpace = {\n",
    "    'Weather':('sun','rain'),    \n",
    "    'Weather_next':('sun','rain'),\n",
    "}\n",
    "\n",
    "# The start state, in this case let's assume we start in a sunny day\n",
    "weatherStart = Factor(('Weather',), outcomeSpace)\n",
    "weatherStart['sun'] = 1.0\n",
    "weatherStart['rain'] = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Now, it is your turn. Define the transition probability table according to the figure above. We have created an initial table for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weatherTransition = Factor(('Weather', 'Weather_next'), outcomeSpace)\n",
    "weatherTransition['sun', 'sun'] = ...\n",
    "weatherTransition['sun', 'rain'] = ...\n",
    "weatherTransition['rain', 'sun'] = ...\n",
    "weatherTransition['rain', 'rain'] = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can double-check our work with the `graphviz` library. The source code bellow draws a state transition graph based on the `weatherTransition` table that you just defined. Compare the `graphviz` plot with the slide figure to confirm your state transition probabilities are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot = Digraph(engine=\"neato\", comment='Weather Markov chain')\n",
    "dot.attr(overlap=\"false\", splines=\"false\")\n",
    "\n",
    "pos = {\n",
    "    'sun': '1,1!',\n",
    "    'rain': '0,0!',    \n",
    "}\n",
    "\n",
    "for v in outcomeSpace['Weather']:\n",
    "    dot.node(v, pos=pos[v])\n",
    "\n",
    "for v in outcomeSpace['Weather']:\n",
    "    for w in outcomeSpace['Weather']:\n",
    "        dot.edge(v, w, str(weatherTransition[v,w]))\n",
    "\n",
    "dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now implement the mini-forward algoritm for Markov chains. Remember from the slides that the update rule for the forward simulation is the following:\n",
    "\n",
    "$P(x_t) = \\sum_{x_{t-1}}P(x_t | x_{t-1})P(x_{t-1})$\n",
    "\n",
    "- $P(x_{t-1})$ is the previous state probability table.\n",
    "\n",
    "- $P(x_t | x_{t-1})$ is the transition probability table such as the one you defined in the previous exercise.\n",
    "\n",
    "- $P(x_t)$ is the current state probability table.\n",
    "\n",
    "From this equation, it is clear we need two basic operations. One is the multiplication of factors (probability tables) that we implemented in a previous tutorial as a `joint` operation. The second one summing one variable out that we implemented as a `marginalize` operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Let's implement the mini-Forward algorithm for Markov chains. We start with the online version that makes a single update (one time step). We use the online version to implement the batch one that runs multiple time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " class MarkovModel():\n",
    "    def __init__(self, start_state, transition, variable_remap):\n",
    "        '''\n",
    "        Takes 3 arguments:\n",
    "        - start_state: a factor representing the start state. E.g. domain might be ('A', 'B', 'C')\n",
    "        - transition: a factor that represents the transition probs. E.g. P('A_next', 'B_next', 'C_next' | 'A', 'B', 'C')\n",
    "        - variable_remap: a dictionary that maps new variable names to old variable names,\n",
    "                          to reset the state after transition. E.g. {'A_next':'A', 'B_next':'B', 'C_next':'C'}\n",
    "        '''\n",
    "        self.state = start_state\n",
    "        self.transition = transition\n",
    "        self.remap = variable_remap\n",
    "\n",
    "    def forward(self):\n",
    "        # get state vars (to be marginalized later)\n",
    "        state_vars = self.state.domain\n",
    "        # join with transition factor\n",
    "        f = ... # TODO 1 line\n",
    "        # marginalize out old state vars, leaving only new state vars\n",
    "        ... # TODO 2 lines\n",
    "        # remap variables to their original names\n",
    "        f.domain = tuple(self.remap[var] for var in f.domain)\n",
    "        self.state = f\n",
    "        return self.state\n",
    "\n",
    "\n",
    "##################\n",
    "# Test code\n",
    "\n",
    "variable_remap = {\n",
    "    \"Weather_next\": \"Weather\"\n",
    "}\n",
    "mm = MarkovModel(weatherStart, weatherTransition, variable_remap)\n",
    "\n",
    "print(mm.forward())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your implementation is correct, you should have the following output:\n",
    "\n",
    "```\n",
    "╒═══════════╤══════╕\n",
    "│ Weather   │   Pr │\n",
    "╞═══════════╪══════╡\n",
    "│ sun       │  0.9 │\n",
    "├───────────┼──────┤\n",
    "│ rain      │  0.1 │\n",
    "╘═══════════╧══════╛\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the mini-forward online implementation to make a simple batch extension that calls the online subroutine a fixed number of times.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Implement the batch version of the mini-forward algorithm for Markov chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovModel(MarkovModel):\n",
    "    def forwardBatch(self, n):\n",
    "        ''' Do `n` steps, and return history list of states '''\n",
    "        history = []\n",
    "        for i in range(n):\n",
    "            ... # TODO forward\n",
    "            ... # TODO append current state to history\n",
    "        return history\n",
    "\n",
    "\n",
    "##################\n",
    "# Test code\n",
    "\n",
    "variable_remap = {\n",
    "    \"Weather_next\": \"Weather\"\n",
    "}\n",
    "mm = MarkovModel(weatherStart, weatherTransition, variable_remap)\n",
    "\n",
    "history = mm.forwardBatch(3)\n",
    "for factor in history:\n",
    "    print(factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you implemented your code correctly, you should see the following output:\n",
    "\n",
    "```\n",
    "╒═══════════╤══════╕\n",
    "│ Weather   │   Pr │\n",
    "╞═══════════╪══════╡\n",
    "│ sun       │  0.9 │\n",
    "├───────────┼──────┤\n",
    "│ rain      │  0.1 │\n",
    "╘═══════════╧══════╛\n",
    "\n",
    "╒═══════════╤══════╕\n",
    "│ Weather   │   Pr │\n",
    "╞═══════════╪══════╡\n",
    "│ sun       │ 0.84 │\n",
    "├───────────┼──────┤\n",
    "│ rain      │ 0.16 │\n",
    "╘═══════════╧══════╛\n",
    "\n",
    "╒═══════════╤═══════╕\n",
    "│ Weather   │    Pr │\n",
    "╞═══════════╪═══════╡\n",
    "│ sun       │ 0.804 │\n",
    "├───────────┼───────┤\n",
    "│ rain      │ 0.196 │\n",
    "╘═══════════╧═══════╛\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now test the convergence of some Markov chains. Remember from the lectures that chains were classified according to two properties:\n",
    "\n",
    "1. Reducibility. An irreducible (or regular) chain has the property that every state is reachable from every state. Therefore the chain has a single stationary distribution. A reducible chain has not such property and, therefore, can have multiple stationary distributions. The general idea is that, if a chain has, say, two disconnected sets of states **A** and **B** and we start in a state $a \\in \\textbf{A}$, then we will never reach a state in **B**. In this case, the stationary distribution will depend on the transition probabilities of the states in **A**. The same occurs if we start in a state $b \\in \\textbf{B}$.\n",
    "\n",
    "2. Periodicity. An irreducible chain is not guaranteed to convergence. To ensure convergence, we need an additional property: aperiodicity. An aperiodic chain avoids alternating forever between states without ever settling in a stationary distribution. A practical issue in that, although irreducible aperiodic chains are guaranteed to converge to a single stationary distribution, the convergence can be very slow, depending on the transition probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Let's implement an additional method `forwardUntilConvergence` that runs until convergence or a maximum number of iterations. We will establish convergence when the error between two consecutive state probability distributions is smaller than a threshold.\n",
    "\n",
    "Let's start with the implementation of `forwardUntilConvergence`. We implemented a helper function `factorError` that computes the absolute error between two factors. The error can be throught of as a measure of distance between two factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def factorError(f1, f2):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `f1`, factor with the current state probability distribution in the chain.\n",
    "    `f2`, factor with the previous state probability distribution in the chain.\n",
    "    \n",
    "    Returns absolute error between f1 and f2. \n",
    "    \"\"\"\n",
    "    assert f1.domain == f2.domain\n",
    "    return np.sum(np.abs(f1.table - f2.table))\n",
    "\n",
    "class MarkovModel(MarkovModel):\n",
    "    def forwardUntilConvergence(self, max_iters=1000, eps=0.000001):\n",
    "        '''\n",
    "        Arguments:\n",
    "        `n`, maximum number of time updates.\n",
    "        `eps`, error threshold to determine convergence.\n",
    "        Returns:\n",
    "        A history of error values\n",
    "        A factor that represents the current state of the chain after n time steps or the convergence error is less than eps.\n",
    "        '''\n",
    "        errors = []\n",
    "        prevState = self.state\n",
    "        for i in range(max_iters):\n",
    "            newState = ... # TODO forward\n",
    "            ... # TODO calculate error\n",
    "            ... # TODO break loop if error is small\n",
    "            ... # TODO append error to errors list\n",
    "            prevState = newState\n",
    "        return errors, newState\n",
    "\n",
    "##################\n",
    "# Test code\n",
    "        \n",
    "variable_remap = {\n",
    "    \"Weather_next\": \"Weather\"\n",
    "}\n",
    "mm = MarkovModel(weatherStart, weatherTransition, variable_remap)\n",
    "errors, state = mm.forwardUntilConvergence()\n",
    "print(state)\n",
    "plt.plot(errors, 'ro')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you implemented your code correctly, you should see an output like this:\n",
    "\n",
    "```\n",
    "╒═══════════╤══════════╕\n",
    "│ Weather   │       Pr │\n",
    "╞═══════════╪══════════╡\n",
    "│ sun       │ 0.750001 │\n",
    "├───────────┼──────────┤\n",
    "│ rain      │ 0.249999 │\n",
    "╘═══════════╧══════════╛\n",
    "```\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Now, we can test the convergence of some Markov chains of the previous tutorial. \n",
    "\n",
    "The first one has the following transition matrix:\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "1/2 & 0 & 1/2 & 0 \\\\\n",
    "0 & 1/2 & 0 & 1/2 \\\\\n",
    "0 & 0 & 1 & 0 \\\\\n",
    "\\end{bmatrix}$\n",
    "    \n",
    "This chain is irreducible since we can reach any state from any state. However, it is periodic since if we start from state 1 in time 1, we reach states 1 and 3 at odd times and 2 and 4 at even times. Its stationary distribution is $(1/6, 2/6, 2/6, 1/6)$.\n",
    "\n",
    "Use the implemented function to confirm that this chain does not converge. Use the next cell to define the necessary variables and call the `forwardUntilConvergence` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomeSpace_c1 = {\n",
    "    'R':(0,1,2,3),\n",
    "    'R_next':(0,1,2,3),\n",
    "}\n",
    "\n",
    "\n",
    "t = Factor(('R', 'R_next'), outcomeSpace_c1)\n",
    "t[0, 0] = 0.0\n",
    "t[0, 1] = 1.0\n",
    "t[0, 2] = 0.0\n",
    "t[0, 3] = 0.0\n",
    "t[1, 0] = 0.5\n",
    "t[1, 1] = 0.0\n",
    "t[1, 2] = 0.5\n",
    "t[1, 3] = 0.0\n",
    "... # TODO finish transition factor\n",
    "\n",
    "\n",
    "s = Factor(('R',), outcomeSpace_c1)\n",
    "s[0] = 1.0\n",
    "s[1] = 0.0\n",
    "s[2] = 0.0\n",
    "s[3] = 0.0\n",
    "\n",
    "\n",
    "##################\n",
    "# Test code\n",
    "\n",
    "variable_remap = {\n",
    "    \"R_next\": \"R\"\n",
    "}\n",
    "mm = MarkovModel(s, t, variable_remap)\n",
    "errors, state = mm.forwardUntilConvergence()\n",
    "print(state)\n",
    "plt.plot(errors, 'ro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second one has the following transition matrix:\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "1/2 & 1/2 & 0 & 0 \\\\\n",
    "1/2 & 0 & 1/2 & 0 \\\\\n",
    "0 & 1/2 & 0 & 1/2 \\\\\n",
    "0 & 0 & 0 & 1 \\\\\n",
    "\\end{bmatrix}$\n",
    "    \n",
    "This chain is reducible since state 4 is absorbing (once entered, cannot be left) and aperiodic. Its stationary distribution is $(0, 0, 0, 1)$.\n",
    "\n",
    "Use the implemented function to verify this chain convergence. Use the next cell to define the necessary variables and call `forwardUntilConvergence` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomeSpace_c2 = {\n",
    "    'R':(0,1,2,3),\n",
    "    'R_next':(0,1,2,3),\n",
    "}\n",
    "\n",
    "t = Factor(('R', 'R_next'), outcomeSpace_c2)\n",
    "# We can set the values in the factor in a different way by directly setting the table\n",
    "# This is quicker, but you have to be careful that the domain is in the right order\n",
    "t.table = np.array(\n",
    "    [\n",
    "     [.5, .5, 0, 0],\n",
    "    ... # TODO finish array\n",
    "    ]\n",
    ")\n",
    "\n",
    "s = Factor(('R',), outcomeSpace_c2)\n",
    "s.table = np.array(\n",
    "    [1,0,0,0]\n",
    ")\n",
    "\n",
    "##################\n",
    "# Test code\n",
    "\n",
    "variable_remap = {\n",
    "    \"R_next\": \"R\"\n",
    "}\n",
    "mm = MarkovModel(s, t, variable_remap)\n",
    "errors, state = mm.forwardUntilConvergence()\n",
    "print(state)\n",
    "plt.plot(errors, 'ro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third one has the following transition matrix:\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "1/2 & 1/2 & 0 & 0 \\\\\n",
    "1/2 & 1/2 & 0 & 0 \\\\\n",
    "0 & 0 & 1/2 & 1/2 \\\\\n",
    "0 & 0 & 1/2 & 1/2 \\\\\n",
    "\\end{bmatrix}$\n",
    "    \n",
    "This chain is reducible since we cannot reach states {3,4} from states {1,2} and vice-versa and aperiodic. It has two stationary distributions: $(1/2, 1/2, 0, 0)$ and $(0, 0, 1/2, 1/2)$.\n",
    "\n",
    "Use the implemented function to verify this chain convergence. Use the next cell to define the necessary variables and call `forwardUntilConvergence` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomeSpace_c3 = {\n",
    "    'R':(0,1,2,3),\n",
    "    'R_next':(0,1,2,3),\n",
    "}\n",
    "\n",
    "t = Factor(('R', 'R_next'), outcomeSpace_c3)\n",
    "t.table = np.array(\n",
    "    [[.5, .5, 0, 0],\n",
    "     [.5, .5, 0, 0],\n",
    "    ... # TODO\n",
    "    ]\n",
    ")\n",
    "\n",
    "s = Factor(('R',), outcomeSpace_c3)\n",
    "s.table = np.array(\n",
    "    [1,0,0,0]\n",
    ")\n",
    "\n",
    "##################\n",
    "# Test code\n",
    "\n",
    "variable_remap = {\n",
    "    \"R_next\": \"R\"\n",
    "}\n",
    "mm = MarkovModel(s, t, variable_remap)\n",
    "errors, state = mm.forwardUntilConvergence()\n",
    "print(state)\n",
    "plt.plot(errors, 'ro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we have a chain in the form:\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "1-e & e \\\\\n",
    "e & 1-e \\\\\n",
    "\\end{bmatrix}$\n",
    "    \n",
    "This chain is irreducible and aperiodic, but its convergence can be very slow for small values of $e$. It has one stationary distribution: $(1/2, 1/2)$.\n",
    "\n",
    "Use the implemented function to verify the chain convergence. Change the value of $e$. Use the next cell to define the necessary variables and call `forwardUntilConvergence` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = 0.0001\n",
    "\n",
    "outcomeSpace_c4 = {\n",
    "    'R':(0,1),\n",
    "    'R_next':(0,1),\n",
    "}\n",
    "\n",
    "t = Factor(('R', 'R_next'), outcomeSpace_c4)\n",
    "t.table = ... # TODO\n",
    "\n",
    "s = Factor(('R',), outcomeSpace_c4)\n",
    "s.table = np.array(\n",
    "    [1,0]\n",
    ")\n",
    "\n",
    "##################\n",
    "# Test code\n",
    "\n",
    "variable_remap = {\n",
    "    \"R_next\": \"R\"\n",
    "}\n",
    "mm = MarkovModel(s, t, variable_remap)\n",
    "errors, state = mm.forwardUntilConvergence()\n",
    "print(state)\n",
    "plt.plot(errors, 'ro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# PageRank algorithm\n",
    "\n",
    "The PageRank algorithm is the original algorithm used by Google 1.0. It can be interpreted as a direct application of Markov chains. \n",
    "\n",
    "PageRank models the web as a state graph: pages are states and hyperlinks are transitions. The transition probabilities as set as follows:\n",
    "\n",
    "Imagine you are surfing the web. \n",
    "* With 85% probability, you will jump to the next website by randomly selecting from the links on that page. \n",
    "* With 15% probability, you will jump randomly to any random page on the graph. \n",
    "* If you are at a page with no links, you will always jump to a random page on the graph.\n",
    "\n",
    "Using these transition probabilities, the PageRank algorithm estimates the stationary distribution over webpages, and uses the probabilities of this distribution as an estimate of the \"popularity\" of every webpage.\n",
    "\n",
    "The following figure from [Wikipedia](https://en.wikipedia.org/wiki/PageRank) illustrates the final probabilities (in percentages) of PageRank for a small graph.\n",
    "\n",
    "![](https://raw.githubusercontent.com/UNSW-COMP9418/Week05/main/img/page_rank.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the mini-forward algorithm to implement the PageRank algorithm. We will use this figure to compare our results. We start by defining this graph. We will do it for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha sets the weight portion reserved for regular edges\n",
    "alpha = .85\n",
    "\n",
    "outcomePageRank = {\n",
    "    \"P\":('A','B','C','D','E','F','G','H','I','J','K'),\n",
    "    \"P_next\":('A','B','C','D','E','F','G','H','I','J','K'),\n",
    "}\n",
    "\n",
    "page_graph = Graph({\n",
    "    \"A\" : [],\n",
    "    'B' : ['C'],\n",
    "    'C' : ['B'],\n",
    "    'D' : ['B','A'],\n",
    "    'E' : ['B','D','F'],\n",
    "    'F' : ['B','E'],\n",
    "    'G' : ['B','E'],\n",
    "    'H' : ['B','E'],\n",
    "    'I' : ['B','E'],\n",
    "    'J' : ['E'],\n",
    "    'K' : ['E'],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Fill in the function below to create a transition table according to the rules above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTransitionTable(graph, outcomePageRank, alpha=0.85):\n",
    "    N = len(graph)\n",
    "    phantomTransition = Factor(('P', 'P_next'), outcomePageRank)\n",
    "    \n",
    "    # for every pair of nodes in the graph\n",
    "    for u in outcomePageRank['P']:\n",
    "        for v in outcomePageRank['P_next']:\n",
    "            # find the number of outgoing links\n",
    "            out_degree = len(graph.children(u))\n",
    "            if out_degree == 0:\n",
    "                # starting at a node 'u' with no neighbors, what's the probability of jumping to v?\n",
    "                prob_v_given_u = ... # TODO\n",
    "            else:\n",
    "                if v in graph.children(u):\n",
    "                    # starting at 'u', the probability of jumping to a connected node 'v'. \n",
    "                    # (Make sure you take into account the 15% possibility of randomly jumping to any node)\n",
    "                    prob_v_given_u = ... # TODO\n",
    "                else:\n",
    "                    # starting at 'u', the probability of jumping to a random node 'v'\n",
    "                    prob_v_given_u = ... # TODO \n",
    "            phantomTransition[u,v] = prob_v_given_u\n",
    "    return phantomTransition\n",
    "            \n",
    "pageRankTransition = createTransitionTable(page_graph, outcomePageRank)\n",
    "print(pageRankTransition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use ``GraphViz`` again to plot this graph and ensure we have designed it correctly. Note that we will not print all $11^2$ conditional probabilities, only the ones that correspond to following a link in the above graph. For a node with one outgoing edge, that edge should have a probability of $0.85 + 0.15\\times(1/11) \\approx 0.864 $, which is the sum of the probability of following the link $85$% and the probability of randomly choosing the connected node $15\\%\\times(1/11)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dot = Digraph(engine=\"neato\", comment='PageRank Markov chain')\n",
    "dot.attr(overlap=\"false\", splines=\"false\", strict=\"true\")\n",
    "\n",
    "pos = {\n",
    "    'A': '0,2!',\n",
    "    'B': '2,2!',\n",
    "    'C': '4,2!',\n",
    "    'D': '0,1!',\n",
    "    'E': '3,1!',\n",
    "    'F': '4,1!',\n",
    "    'G': '0,0!',\n",
    "    'H': '1,0!',\n",
    "    'I': '2,0!',\n",
    "    'J': '3,0!',\n",
    "    'K': '4,0!',\n",
    "}\n",
    "\n",
    "for v in page_graph:\n",
    "    dot.node(v, pos=pos[v])\n",
    "\n",
    "for v in page_graph:\n",
    "    for w in page_graph.children(v):\n",
    "        dot.edge(v, w, str(round(pageRankTransition[v,w],3)))\n",
    "\n",
    "dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_remap = {\n",
    "    \"P_next\": \"P\"\n",
    "}\n",
    "\n",
    "# We set all states with equal initial probabilities\n",
    "startPageRank = Factor(('P',), outcomePageRank)\n",
    "startPageRank['A'] = 1/11\n",
    "startPageRank['B'] = 1/11\n",
    "startPageRank['C'] = 1/11\n",
    "startPageRank['D'] = 1/11\n",
    "startPageRank['E'] = 1/11\n",
    "startPageRank['F'] = 1/11\n",
    "startPageRank['G'] = 1/11\n",
    "startPageRank['H'] = 1/11\n",
    "startPageRank['I'] = 1/11\n",
    "startPageRank['J'] = 1/11\n",
    "startPageRank['K'] = 1/11\n",
    "\n",
    "pageRankTransition = createTransitionTable(page_graph, outcomePageRank)\n",
    "\n",
    "mm = MarkovModel(startPageRank, pageRankTransition, variable_remap)\n",
    "errors, page_rank = mm.forwardUntilConvergence()\n",
    "print(page_rank)\n",
    "plt.plot(errors, 'ro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use ``GraphViz`` to visualise the results of the PageRank algorithm implementation. The next cell tries to mimic the Wikipedia figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot = Digraph(engine=\"neato\", comment='PageRank Markov chain')\n",
    "dot.attr(overlap=\"false\", splines=\"false\", strict=\"true\")\n",
    "\n",
    "pos = {\n",
    "    'A': '0,2!',\n",
    "    'B': '2,2!',\n",
    "    'C': '4,2!',\n",
    "    'D': '0,1!',\n",
    "    'E': '3,1!',\n",
    "    'F': '4,1!',\n",
    "    'G': '0,0!',\n",
    "    'H': '1,0!',\n",
    "    'I': '2,0!',\n",
    "    'J': '3,0!',\n",
    "    'K': '4,0!',\n",
    "}\n",
    "\n",
    "for v in page_graph:\n",
    "    dot.node(v, pos=pos[v], label=v+'\\n'+str(round(page_rank[v]*100,1)))\n",
    "\n",
    "for v in page_graph:\n",
    "    for w in page_graph.children(v):\n",
    "        dot.edge(v, w)\n",
    "\n",
    "dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/UNSW-COMP9418/Week05/main/img/page_rank.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Markov Models (HMMs)\n",
    "\n",
    "We now turn our attention to HMMs. These are popular dynamic Bayesian Networks. We will implement the forward and Viterbi algorithms and verify how these algorithms provide answers to the questions on the theory part of this tutorial.\n",
    "\n",
    "We start with the forward algorithm. According to the lecture slides, the forward algorithm for HMMs has the following form:\n",
    "\n",
    "1. The transition step is similar to Markov chains. In this step, we have a passage of time and the distribution moves towards the stationary distribution.\n",
    "\n",
    "    $P(x_t|e_{1:t-1}) = \\sum_{x_{t-1}}P(x_{t-1}|e_{1:t-1})P(x_t|x_{t-1})$\n",
    "\n",
    "    where, $P(x_t|e_{1:t-1})$ is the current state before observing the evidence $e_t$. $P(x_{t-1}|e_{1:t-1})$ is the previous state and $P(x_t|x_{t-1})$ is the transition probability.\n",
    "\n",
    "\n",
    "2. The emission step has the following form:\n",
    "\n",
    "    $P(x_t|e_{1:t}) \\propto P(x_t|e_{1:t-1})P(e_t|x_t)$\n",
    "\n",
    "    where, $P(x_t|e_{1:t-1})$ is the current state after observing the evidence $e_t$. $P(x_t|e_{1:t-1})$ is the current state before observing de evidence (obtained in the previous step) and $P(e_t|x_t)$ is the emission probability.\n",
    "\n",
    "\n",
    "The symbol $\\propto$ means that the emission step requires a normalization. The normalization is necessary because we are omitting the denominator in the emission update. The denominator is the probability of the evidence, which we frequently do not have readily available. Although we can compute such quantity, the normalization is usually easier to calculate.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Let's implement the forward algorithm for HMMs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make this function more flexible, we will allow the user to pass an empty emission value. This means that no evidence was observed in this step. Also, the renormalization will be optional, so we will keep it commented for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenMarkovModel():\n",
    "    def __init__(self, start_state, transition, emission, variable_remap):\n",
    "        '''\n",
    "        Takes 3 arguments:\n",
    "        - start_state: a factor representing the start state. E.g. domain might be ('A', 'B', 'C')\n",
    "        - transition: a factor that represents the transition probs. E.g. P('A_next', 'B_next', 'C_next' | 'A', 'B', 'C')\n",
    "        - emission: emission probabilities. E.g. P('O' | 'A', 'B', 'C')\n",
    "        - variable_remap: a dictionary that maps new variable names to old variable names,\n",
    "                            to reset the state after transition. E.g. {'A_next':'A', 'B_next':'B', 'C_next':'C'}\n",
    "        '''\n",
    "        self.state = start_state\n",
    "        self.transition = transition\n",
    "        self.emission = emission\n",
    "        self.remap = variable_remap\n",
    "\n",
    "        # These lists will be used later to find the mostly likely sequence of states\n",
    "        self.history = []\n",
    "        self.prev_history = []\n",
    "\n",
    "    def forward(self, **emission_evi):\n",
    "        # get state vars (to be marginalized later)\n",
    "        state_vars = self.state.domain\n",
    "\n",
    "        # join with transition factor\n",
    "        f = ... # TODO\n",
    "\n",
    "        # marginalize out old state vars, leaving only new state vars\n",
    "        for var in state_vars:\n",
    "            f = ... # TODO\n",
    "\n",
    "        # remap variables to their original names\n",
    "        f.domain = tuple(self.remap[var] for var in f.domain)\n",
    "        self.state = f\n",
    "\n",
    "        # set emission evidence\n",
    "        emissionFactor = ... # TODO\n",
    "\n",
    "        # join with state factor\n",
    "        f = ... # TODO\n",
    "\n",
    "        # marginalize out emission vars\n",
    "        for var in f.domain:\n",
    "            if var not in state_vars:\n",
    "                f = ... # TODO\n",
    "        self.state = f\n",
    "\n",
    "        # normalize state (keep commented out for now)\n",
    "        # self.state = self.state.normalize()\n",
    "\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the weather example from the lecture to test our implementation. In this example, a graduate student tries to figure out the current weather state by observing this advisor carrying an umbrella.\n",
    "\n",
    "These are the transition and emission probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "# Test code\n",
    "\n",
    "from collections import OrderedDict as odict\n",
    "\n",
    "# possible outcomes, by variable\n",
    "outcomeSpace = {\n",
    "    'Weather':('sun','rain'),\n",
    "    'Weather_next':('sun','rain'),\n",
    "    'Umbrella': ('umbrella', 'no_umbrella'),\n",
    "}\n",
    "\n",
    "weatherStart = Factor(('Weather',), outcomeSpace)\n",
    "weatherStart['sun'] = 0.5\n",
    "weatherStart['rain'] = 0.5\n",
    "weatherTransition = Factor(('Weather', 'Weather_next'), outcomeSpace)\n",
    "weatherTransition['sun', 'sun'] = 0.7\n",
    "weatherTransition['sun', 'rain'] = 0.3\n",
    "weatherTransition['rain', 'sun'] = 0.3\n",
    "weatherTransition['rain', 'rain'] = 0.7\n",
    "weatherEmission = Factor(('Weather', 'Umbrella'), outcomeSpace)\n",
    "weatherEmission['sun', 'umbrella'] = 0.2\n",
    "weatherEmission['sun', 'no_umbrella'] = 0.8\n",
    "weatherEmission['rain', 'umbrella'] = 0.9\n",
    "weatherEmission['rain', 'no_umbrella'] = 0.1\n",
    "\n",
    "variable_remap = {\n",
    "    'Weather_next': 'Weather'\n",
    "}\n",
    "\n",
    "hmm = HiddenMarkovModel(weatherStart, weatherTransition, weatherEmission, variable_remap)\n",
    "print(hmm.forward(Umbrella='umbrella'))\n",
    "\n",
    "hmm = HiddenMarkovModel(weatherStart, weatherTransition, weatherEmission, variable_remap)\n",
    "print(hmm.forward())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you implemented your code correctly, you should see an output like this:\n",
    "\n",
    "```\n",
    "╒═══════════╤══════╕\n",
    "│ Weather   │   Pr │\n",
    "╞═══════════╪══════╡\n",
    "│ sun       │ 0.1  │\n",
    "├───────────┼──────┤\n",
    "│ rain      │ 0.45 │\n",
    "╘═══════════╧══════╛\n",
    "\n",
    "╒═══════════╤══════╕\n",
    "│ Weather   │   Pr │\n",
    "╞═══════════╪══════╡\n",
    "│ sun       │  0.5 │\n",
    "├───────────┼──────┤\n",
    "│ rain      │  0.5 │\n",
    "╘═══════════╧══════╛\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Let's now implement the batch version of the forward algorithm. The batch version takes as input a sequence of `n` observations and outputs an array of length `n` with the state distribution at each time step.\n",
    "\n",
    "We have created a stub for you. You should pass as argument a list of evidence observations. Call the online version of the algorithm as many times as you have items in the emissionEviList."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenMarkovModel(HiddenMarkovModel):\n",
    "    def forwardBatch(self, n, **emission_evi):\n",
    "        '''\n",
    "        emission_evi: A dictionary of lists, each list containing the evidence list for a variable. \n",
    "                         Use `None` if no evidence for that timestep\n",
    "        '''\n",
    "        history = []\n",
    "        for i in range(n):\n",
    "            # select evidence for this timestep\n",
    "            evi_dict = dict([(key, value[i]) for key, value in emission_evi.items() if value[i] is not None])\n",
    "            \n",
    "            # take a step forward\n",
    "            state = ... # TODO\n",
    "            history.append(state)\n",
    "        return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm = HiddenMarkovModel(weatherStart, weatherTransition, weatherEmission, variable_remap)\n",
    "timeLine = hmm.forwardBatch(2, Umbrella=['umbrella', 'umbrella'])\n",
    "for t in range(len(timeLine)):\n",
    "    print(\"Time: \", t)\n",
    "    print(timeLine[t])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your implementation is correct. You should see an output like this one:\n",
    "\n",
    "```\n",
    "Time:  0\n",
    "╒═══════════╤══════╕\n",
    "│ Weather   │   Pr │\n",
    "╞═══════════╪══════╡\n",
    "│ sun       │ 0.1  │\n",
    "├───────────┼──────┤\n",
    "│ rain      │ 0.45 │\n",
    "╘═══════════╧══════╛\n",
    "\n",
    "\n",
    "Time:  1\n",
    "╒═══════════╤════════╕\n",
    "│ Weather   │     Pr │\n",
    "╞═══════════╪════════╡\n",
    "│ sun       │ 0.041  │\n",
    "├───────────┼────────┤\n",
    "│ rain      │ 0.3105 │\n",
    "╘═══════════╧════════╛\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's implement the Viterbi algorithm. The Viterbi algorithm provides answers to queries in the form of the most likely explanation (MLE). In other words, the output will be the most likely instantiation for each of the hidden states.\n",
    "\n",
    "According to the course slides, the Viterbi algorithm has the following equations:\n",
    "\n",
    "$m_t[x_t] = P(e_t|x_t) max_{x_{t-1}}P(x_t|x_{t-1})m_{t-1}[x_{t-1}]$\n",
    "\n",
    "where, \n",
    "\n",
    "- $m_t[x_t]$ is the MLE for time $t$.\n",
    "- $P(e_t|x_t)$ is the emission probability.\n",
    "- $P(x_t|x_{t-1})$ is the transition probability.\n",
    "- $m_{t-1}[x_{t-1}]$ is the MLE for time $t-1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "To implement the Viterbi algorithm, we will need an intermediate operation `maximize`. Such an operation is similar to `marginalize` in the sense that it will eliminate one variable of a factor. However, instead of summing out this variable, we will compute the maximum among the entries.\n",
    "\n",
    "We have created a stub for you. You will need to complete a few gaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "class Factor(Factor):\n",
    "    def maximize(self, var, return_prev=False):\n",
    "        '''\n",
    "        Usage: f.maximize('B'), where 'B' is a variable name.\n",
    "        This function removes a variable from the domain, and maximizes over that variable in the table\n",
    "        The return_prev argument will be used when tracing back the viterbi algorithm\n",
    "        '''\n",
    "        \n",
    "        # create new domain\n",
    "        ... # TODO\n",
    "        \n",
    "        # remove an axis of the table by taking a maximum over that axis\n",
    "        axis = self.domain.index(var)\n",
    "        new_table = ... # TODO\n",
    "        \n",
    "        # create a new factor to be returned\n",
    "        outputFactor = self.__class__(tuple(new_dom),self.outcomeSpace, new_table)\n",
    "\n",
    "        if return_prev:\n",
    "            # get the index chosen when taking maximum (to be used later in the viterbi algorithm)\n",
    "            prev = np.argmax(self.table, axis=axis)\n",
    "            return outputFactor, prev\n",
    "        else:\n",
    "            return outputFactor\n",
    "\n",
    "\n",
    "################\n",
    "# Test code\n",
    "\n",
    "# re-initialize the factors so that it has access to the maximize function\n",
    "weatherStart = Factor(weatherStart.domain, weatherStart.outcomeSpace, weatherStart.table)\n",
    "weatherTransition = Factor(weatherTransition.domain, weatherTransition.outcomeSpace, weatherTransition.table)\n",
    "weatherEmission = Factor(weatherEmission.domain, weatherEmission.outcomeSpace, weatherEmission.table)\n",
    "\n",
    "print(weatherEmission.maximize('Umbrella'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your implementation is correct, you should see the following output:\n",
    "\n",
    "```\n",
    "╒═══════════╤══════╕\n",
    "│ Weather   │   Pr │\n",
    "╞═══════════╪══════╡\n",
    "│ sun       │  0.8 │\n",
    "├───────────┼──────┤\n",
    "│ rain      │  0.9 │\n",
    "╘═══════════╧══════╛\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Now we are in the position to implement the Viterbi algorithm. We have started the implementation and left a few details for you to fill in. As before, we will code the online version first. The online version provides the output for a single time and evidence observation increment. The batch version is a simple extension of the online algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenMarkovModel(HiddenMarkovModel):\n",
    "    def viterbi(self, **emission_evi):\n",
    "        '''\n",
    "        This function is very similar to the forward algorithm. \n",
    "        For simplicity, we will assume that there is only one state variable, and one emission variable.\n",
    "        '''\n",
    "\n",
    "        # confirm that state and emission each have 1 variable \n",
    "        assert len(self.state.domain) == 1\n",
    "        assert len(self.emission.domain) == 2\n",
    "        assert len(self.transition.domain) == 2\n",
    "\n",
    "        # get state and evidence var names (to be marginalized and maximised out later)\n",
    "        state_var_name = self.state.domain[0]\n",
    "        emission_vars = [v for v in self.emission.domain if v not in self.state.domain]\n",
    "        emission_var_name = emission_vars[0]\n",
    "\n",
    "        # join with transition factor\n",
    "        f = ... # TODO\n",
    "\n",
    "        # maximize out old state vars, leaving only new state vars\n",
    "        f, prev = ... # TODO (use return_prev to also return prev)\n",
    "        self.prev_history.append(prev) # save prev for use in traceback\n",
    "\n",
    "        # remap variables to their original names\n",
    "        f.domain = tuple(self.remap[var] for var in f.domain)\n",
    "        self.state = f\n",
    "\n",
    "        # set emission evidence\n",
    "        emissionFactor = ... # TODO\n",
    "\n",
    "        # join with state factor\n",
    "        f = ... # TODO\n",
    "\n",
    "        # marginalize out emission vars\n",
    "        f = ... # TODO\n",
    "        self.state = f\n",
    "\n",
    "        # normalize state (keep commented out for now)\n",
    "        # self.state = self.state.normalize()\n",
    "\n",
    "        self.history.append(self.state)\n",
    "\n",
    "        return self.state\n",
    "\n",
    "    def viterbiBatch(self, n,  **emission_evi):\n",
    "        '''\n",
    "        emission_evi: A dictionary of lists, each list containing the evidence list for a variable. \n",
    "                         Use `None` if no evidence for that timestep\n",
    "        '''\n",
    "        for i in range(n):\n",
    "            # get evidence for this timestep\n",
    "            evi_dict = dict([(key, value[i]) for key, value in emission_evi.items() if value[i] is not None])\n",
    "            ... # TODO take a step using the `viterbi` method\n",
    "        return self.history\n",
    "\n",
    "##################\n",
    "# Test code\n",
    "\n",
    "hmm = HiddenMarkovModel(weatherStart, weatherTransition, weatherEmission, variable_remap)\n",
    "print(hmm.viterbi(Umbrella='umbrella'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your implementation is correct, you should see the following output:\n",
    "\n",
    "```\n",
    "╒═══════════╤═══════╕\n",
    "│ Weather   │    Pr │\n",
    "╞═══════════╪═══════╡\n",
    "│ sun       │ 0.07  │\n",
    "├───────────┼───────┤\n",
    "│ rain      │ 0.315 │\n",
    "╘═══════════╧═══════╛\n",
    "```\n",
    "\n",
    "Now, we can test the batch version of the Viterbi algorithm with a simple code that runs 2 iterations and prints the timeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hmm = HiddenMarkovModel(weatherStart, weatherTransition, weatherEmission, variable_remap)\n",
    "timeLine = hmm.viterbiBatch(2, Umbrella=['umbrella', 'umbrella'])\n",
    "for t in range(len(timeLine)):\n",
    "    print(\"Time: \", t)\n",
    "    print(timeLine[t])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your implementation is correct, you should see the following output:\n",
    "\n",
    "```\n",
    "Time:  0\n",
    "╒═══════════╤═══════╕\n",
    "│ Weather   │    Pr │\n",
    "╞═══════════╪═══════╡\n",
    "│ sun       │ 0.07  │\n",
    "├───────────┼───────┤\n",
    "│ rain      │ 0.315 │\n",
    "╘═══════════╧═══════╛\n",
    "\n",
    "Time:  1\n",
    "╒═══════════╤═════════╕\n",
    "│ Weather   │      Pr │\n",
    "╞═══════════╪═════════╡\n",
    "│ sun       │ 0.0189  │\n",
    "├───────────┼─────────┤\n",
    "│ rain      │ 0.19845 │\n",
    "╘═══════════╧═════════╛\n",
    "```\n",
    "\n",
    "Before we conclude with an exercise, here are some suggestions of ways to improve the source code of this tutorial:\n",
    "\n",
    "1. The forward algorithm typically does not normalize the intermediate results. You can make this step optional with a flag. This change can also be done to the Viterbi algorithm.\n",
    "\n",
    "2. If you do not normalize, the probabilities will assume small values due to the sequence of multiplications. In this case, operating with log-probabilities will decrease the chance of having underflows.\n",
    "\n",
    "3. For simplicity, we are assuming the update steps are composed by a transition followed by emission. But it is not always the case. It would be better if these steps can be inverted or, even better, implemented independently.\n",
    "\n",
    "## Finding the MPE Assignment\n",
    "\n",
    "As we discussed in the lecture, we can find the MPE assigment from the output computed by the Viterbi algorithm. Remember, that the correct assignment is obtained when we trace back the computations, starting with the last state and move towards the first state. There are two main approaches for finding the MPE assignment:\n",
    "\n",
    "1. Use an additional data structure to store pointers indicating the path of the highest probability.\n",
    "\n",
    "2. Use the output of the Viterbi algorithm and trace back the computations.\n",
    "\n",
    "In the next cell, we provide a function to find the MPE assignment using the output of the ``viterbi`` function. We will use the first method of finding the MPE assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenMarkovModel(HiddenMarkovModel):\n",
    "    def traceBack(self):\n",
    "        '''\n",
    "        This function iterates backwards over the history to find the most \n",
    "        likely sequence of states.\n",
    "        For simplicity, this function assumes there is one state variable\n",
    "        '''\n",
    "        # get most likely outcome of final state\n",
    "        index = np.argmax(self.history[-1].table)\n",
    "        \n",
    "        # Go through \"prev_history\" in reverse\n",
    "        indexList = []\n",
    "        for prev in reversed(self.prev_history):\n",
    "            indexList.append(index)\n",
    "            index = prev[index]\n",
    "        indexList = reversed(indexList)\n",
    "\n",
    "        # translate the indicies into the outcomes they represent\n",
    "        mleList = []\n",
    "        stateVar = self.state.domain[0]\n",
    "        for idx in indexList:\n",
    "            mleList.append(self.state.outcomeSpace[stateVar][idx]) \n",
    "        return mleList\n",
    "\n",
    "####################\n",
    "## Test code\n",
    "\n",
    "hmm = HiddenMarkovModel(weatherStart, weatherTransition, weatherEmission, variable_remap)\n",
    "timeLine = hmm.viterbiBatch(2, Umbrella=['umbrella', 'umbrella'])\n",
    "mpe = hmm.traceBack()\n",
    "print(mpe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "You can use the implemented code to find the numerical answers to the questions of the theory part of this tutorial.\n",
    "\n",
    "### Question 1\n",
    "\n",
    "Lisa is given a fair coin $C_1$ and asked to flip it eight times in a row. Lisa also has a biased coin $C_2$ with probability 0.8 of landing heads. All we know is that Lisa flipped the fair coin initially but we believe that she intends to switch to the biased coin and that she tends to be 10% successful in performing the switch. Suppose that we observe the outcome of the eight coin flips and want to find out whether Lisa managed to perform a coin switch and when. What is the solution to this problem assuming that the flips came out as follows:\n",
    "\n",
    "    a. tails, tails, tails, heads, heads, heads, heads, heads\n",
    "    b. tails, tails, heads, heads, heads, heads, heads, heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Consider a cow that may be infected with a disease that can possibly be detected by performing a milk test. The test is performed on five consecutive days, leading to five outcomes. We want to determine the state of the cow's infection over these days given the test outcomes. The prior probability of an infection on day one is 1/10,000; the test false positive rate is 5/1,000; and its false negative rate is 1/1,000. Moreover, the state of infection at a given day depends only on its state at the previous day. In particular, the probability of a new infection on a given day is 2/10,000, while the probability that an infection would persist to the next day os 7/10.\n",
    "\n",
    "What is the most likely state of the cow's infection over the five days given the following test outcomes:\n",
    "\n",
    "    a. positive, positive, negative, positive, positive\n",
    "    b. positive, negative, negative, positive, positive\n",
    "    c. positive, nagative, negative, negative, positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Task\n",
    "Add the `maximize` method to your DiscreteFactors.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "198px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
